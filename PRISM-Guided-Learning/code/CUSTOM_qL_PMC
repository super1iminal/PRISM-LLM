import numpy as np
import subprocess
import tempfile
import os
from typing import Dict, Tuple, List, Any
from fractions import Fraction
import logging
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import time
import traceback

# Set up logging
logging.basicConfig(level=logging.DEBUG,
                   format='%(asctime)s - %(levelname)s - %(message)s',
                   handlers=[
                       logging.FileHandler("debug.log"),
                       logging.StreamHandler()
                   ])
logger = logging.getLogger(__name__)


        





class LTLGuidedQLearning:
    """Standard Q-Learning with PMC Integration"""
    def __init__(self, size: int, prism_verifier: PrismVerifier):
        self.size = size
        self.env = GridWorldEnv(size)
        self.model_generator = PrismModelGenerator(size)
        self.verifier = SimplifiedVerifier(prism_verifier)
        self.logger = logging.getLogger(__name__)
        
        # Initialize Q-table and parameters
        self.q_table = {}
        self.action_space = 4
        self._initialize_q_table()
        
        # Learning parameters
        self.epsilon = 0.9
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.997
        self.alpha = 0.2
        self.alpha_min = 0.05
        self.gamma = 0.99
        
        # Store PMC verification results
        self.prism_probs = {
            'goal1': 0.0,
            'goal2': 0.0,
            'goal3': 0.0,
            'seq1': 0.0,
            'seq2': 0.0,
            'seq3': 0.0,
            'avoid_obstacle': 0.0,
            'path_exists': 0.0
        }
        
        # Performance tracking
        self.episode_rewards = []
        self.ltl_scores = []
        self.best_policy = None
        self.best_ltl_score = 0.0
        self.training_stats = []

    def _initialize_q_table(self):
        """Initialize Q-table"""
        for x in range(self.size):
            for y in range(self.size):
                for g1 in [False, True]:
                    for g2 in [False, True]:
                        for g3 in [False, True]:
                            state = (x, y, g1, g2, g3)
                            self.q_table[state] = np.ones(self.action_space) * 1.0

    def _get_action(self, state: tuple) -> int:
        """Epsilon-greedy action selection with updated goal positions"""
        if np.random.random() < self.epsilon:
            x, y, g1, g2, g3 = state
            if not g1:
                target = (2, 2)    # Updated Goal 1
            elif not g2:
                target = (3, 3)    # Updated Goal 2
            elif not g3:
                target = (0, 3)    # Updated Goal 3
            else:
                return np.random.randint(self.action_space)
            
            # Simple heuristic for goal-directed movement
            dx = target[0] - x
            dy = target[1] - y
            
            if abs(dx) > abs(dy):
                if dx > 0:
                    preferred_action = 1  # Right
                else:
                    preferred_action = 3  # Left
            else:
                if dy > 0:
                    preferred_action = 2  # Down
                else:
                    preferred_action = 0  # Up

            if np.random.random() < 0.7:
                return preferred_action
            else:
                return np.random.randint(self.action_space)
        return int(np.argmax(self.q_table[state]))

    def _update_prism_probabilities(self, probabilities: List[float]):
        """Update PMC verification probabilities"""
        if len(probabilities) >= 8:
            self.prism_probs = {
                'goal1': probabilities[0],
                'goal2': probabilities[1],
                'goal3': probabilities[2],
                'seq1': probabilities[3],
                'seq2': probabilities[4],
                'seq3': probabilities[5],
                'avoid_obstacle': probabilities[6],
                'path_exists': probabilities[7]
            }

    def _adjust_parameters(self, ltl_score: float):
        """Adjust learning parameters based on verification results"""
        if ltl_score > self.best_ltl_score:
            self.best_ltl_score = ltl_score
            self.best_policy = self.get_current_policy()
            self.epsilon = max(self.epsilon_min, self.epsilon * 0.997)
            self.alpha = max(self.alpha_min, self.alpha * 0.998)
        else:
            self.epsilon = min(0.9, self.epsilon * 1.02)
            
        self.training_stats.append({
            'ltl_score': ltl_score,
            'epsilon': self.epsilon,
            'alpha': self.alpha,
            'best_score': self.best_ltl_score
        })

    def get_current_policy(self) -> Dict[Tuple[int, int, bool, bool, bool], int]:
        """Extract current policy from Q-table"""
        return {state: int(np.argmax(q_values)) 
                for state, q_values in self.q_table.items()}

    def train(self, episodes: int, verify_interval: int = 100):
        """Training Loop with PMC Integration"""
        try:
            start_time = time.time()
            best_episode = 0
            
            for episode in range(episodes):
                state = self.env.reset()
                total_reward = 0
                done = False
                steps = 0
                
                # PMC verification at intervals
                if episode % verify_interval == 0:
                    try:
                        model_str = self.model_generator.generate_prism_model(
                            self.get_current_policy(), self.q_table)
                        ltl_score = self.verifier.verify_policy(model_str)
                        
                        if self.verifier.ltl_probabilities:
                            self._update_prism_probabilities(self.verifier.ltl_probabilities[-1])
                        
                        self.ltl_scores.append(ltl_score)
                        self._adjust_parameters(ltl_score)
                        
                        self.logger.info(
                            f"Episode {episode}: LTL Score = {ltl_score:.4f}, "
                            f"Epsilon = {self.epsilon:.3f}, "
                            f"Alpha = {self.alpha:.3f}, "
                            f"Best Score = {self.best_ltl_score:.4f}, "
                            f"Time = {(time.time() - start_time)/60:.1f}m"
                        )
                        
                        if ltl_score > self.best_ltl_score:
                            best_episode = episode
                            
                    except Exception as e:
                        self.logger.error(f"Verification error in episode {episode}: {str(e)}")
                        continue
                
                while not done:
                    action = self._get_action(state)
                    next_state, reward, done = self.env.step(action)
                    
                    # Standard Q-learning update
                    current_q = self.q_table[state][action]
                    next_max_q = np.max(self.q_table[next_state])
                    new_q = current_q + self.alpha * (
                        reward + self.gamma * next_max_q - current_q
                    )
                    self.q_table[state][action] = new_q
                    
                    state = next_state
                    total_reward += reward
                    steps += 1
                    
                    if steps >= self.env.max_steps:
                        done = True
                
                self.episode_rewards.append(total_reward)
                
                if episode % 10 == 0:
                    self.logger.info(f"Episode {episode}: Steps = {steps}, "
                                   f"Total Reward = {total_reward:.2f}")
            
            # Final verification
            try:
                model_str = self.model_generator.generate_prism_model(
                    self.get_current_policy(), self.q_table)
                final_score = self.verifier.verify_policy(model_str)
                self.logger.info(f"\nFinal LTL Score: {final_score:.4f}")
            except Exception as e:
                self.logger.error(f"Final verification error: {str(e)}")
            
            self.verifier.save_probabilities_to_file(self)
            self.save_training_stats()
            
            training_time = time.time() - start_time
            self.logger.info(f"\nTraining completed in {training_time/60:.1f} minutes")
            self.logger.info(f"Best LTL Score: {self.best_ltl_score:.4f} "
                           f"(Episode {best_episode})")
            
            return self.best_policy, self.best_ltl_score
            
        except Exception as e:
            self.logger.error(f"Training error: {str(e)}")
            self.logger.error(traceback.format_exc())
            raise

    def save_training_stats(self, save_dir: str = "results"):
        """Save training statistics"""
        os.makedirs(save_dir, exist_ok=True)
        stats_file = os.path.join(save_dir, 'training_statistics.txt')
        
        try:
            with open(stats_file, 'w') as f:
                f.write("Episode,LTL_Score,Epsilon,Alpha,Best_Score\n")
                for episode, stats in enumerate(self.training_stats):
                    f.write(f"{episode},"
                           f"{stats['ltl_score']:.4f},"
                           f"{stats['epsilon']:.4f},"
                           f"{stats['alpha']:.4f},"
                           f"{stats['best_score']:.4f}\n")
                           
            self.logger.info(f"Training statistics saved to {stats_file}")
        except Exception as e:
            self.logger.error(f"Failed to save training statistics: {str(e)}")
            
    def plot_training_progress(self, save_dir: str = "results"):
        """Plot training progress metrics"""
        os.makedirs(save_dir, exist_ok=True)
        
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 1, 1)
        episodes = range(len(self.episode_rewards))
        plt.plot(episodes, self.episode_rewards, 'b-', alpha=0.3, label='Rewards')
        
        window = min(10, len(self.episode_rewards))
        if len(self.episode_rewards) >= window:
            smoothed = np.convolve(self.episode_rewards,
                                np.ones(window)/window,
                                mode='valid')
            plt.plot(range(window-1, len(self.episode_rewards)),
                    smoothed, 'r-', label=f'Moving Average (w={window})')
        
        plt.title('Training Rewards')
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.legend()
        plt.grid(True)

        plt.subplot(2, 1, 2)
        verify_episodes = range(0, len(self.training_stats))
        
        plt.plot(verify_episodes,
                [s['ltl_score'] for s in self.training_stats],
                'g-', label='LTL Score')
        plt.plot(verify_episodes,
                [s['epsilon'] for s in self.training_stats],
                'b--', label='Epsilon')
        plt.plot(verify_episodes,
                [s['alpha'] for s in self.training_stats],
                'r--', label='Alpha')
        
        plt.title('LTL Scores and Parameters')
        plt.xlabel('Verification Episode')
        plt.ylabel('Value')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_progress.png'))
        plt.close()

def plot_training_progress(self, save_dir: str = "results"):
    """Plot training progress metrics"""
    os.makedirs(save_dir, exist_ok=True)
    
    plt.figure(figsize=(15, 10))
    
    plt.subplot(2, 1, 1)
    episodes = range(len(self.episode_rewards))
    plt.plot(episodes, self.episode_rewards, 'b-', alpha=0.3, label='Rewards')
    
    window = min(10, len(self.episode_rewards))
    if len(self.episode_rewards) >= window:
        smoothed = np.convolve(self.episode_rewards,
                            np.ones(window)/window,
                            mode='valid')
        plt.plot(range(window-1, len(self.episode_rewards)),
                smoothed, 'r-', label=f'Moving Average (w={window})')
    
    plt.title('Training Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    plt.grid(True)

    plt.subplot(2, 1, 2)
    verify_episodes = range(0, len(self.training_stats))
    
    plt.plot(verify_episodes,
            [s['ltl_score'] for s in self.training_stats],
            'g-', label='LTL Score')
    plt.plot(verify_episodes,
            [s['epsilon'] for s in self.training_stats],
            'b--', label='Epsilon')
    plt.plot(verify_episodes,
            [s['alpha'] for s in self.training_stats],
            'r--', label='Alpha')
    
    plt.title('LTL Scores and Parameters')
    plt.xlabel('Verification Episode')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'training_progress.png'))
    plt.close()

def get_prism_path():
    """Get PRISM executable path"""
    prism_path = "/home/moonstone/Documents/Coding/prism-llm/prism-4.9/bin/prism"  # Update this path for your system
    if os.path.exists(prism_path) and os.access(prism_path, os.X_OK):
        return prism_path
    raise FileNotFoundError(f"PRISM executable not found at {prism_path}")

def main():
    """Main execution function for 10x10 grid world"""
    try:
        # Set up logging
        os.makedirs("logs", exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler("logs/training.log"),
                logging.StreamHandler()
            ]
        )
        logger = logging.getLogger(__name__)
        
        # Initialize PRISM
        try:
            prism_path = get_prism_path()
            logger.info(f"Successfully initialized PRISM at: {prism_path}")
        except FileNotFoundError as e:
            logger.error(f"PRISM initialization failed: {e}")
            return
        
        # Training parameters - adjusted for 10x10 grid
        verify_interval = 1      
        total_episodes = 100     # Adjusted episodes for smaller grid
        grid_size = 4          # Updated grid size
        
        # Create results directory
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)
        
        # Initialize verifier and agent
        logger.info("Initializing verifier and agent...")
        prism_verifier = PrismVerifier(prism_path)
        agent = LTLGuidedQLearning(size=grid_size, 
                                 prism_verifier=prism_verifier)
        
        # Train agent
        logger.info(f"Starting training on {grid_size}x{grid_size} grid")
        logger.info(f"Goals: G1(2,2), G2(3,3), G3(0,3)")
        logger.info(f"Static Obstacles: (1,1), (2,1)")
        logger.info(f"Moving Obstacle Path: (3,0)->(3,1)->(3,2)->(3,3)->(3,2)")
        logger.info(f"Training for {total_episodes} episodes")
        
        start_time = time.time()
        best_policy, best_ltl_score = agent.train(
            episodes=total_episodes, 
            verify_interval=verify_interval
        )
        training_time = time.time() - start_time
        
        # Save final results
        agent.save_training_stats(results_dir)
        agent.verifier.save_probabilities_to_file(agent, results_dir)
        agent.plot_training_progress(results_dir)
        
        # Log final results
        logger.info("\nTraining Summary:")
        logger.info(f"Training time: {training_time/60:.1f} minutes")
        logger.info(f"Best LTL Score: {best_ltl_score:.4f}")
        logger.info(f"Results saved to {results_dir}")
        
    except Exception as e:
        logger.error(f"Critical error in main execution: {str(e)}")
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
